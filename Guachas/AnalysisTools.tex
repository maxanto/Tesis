\subsubsection{Analysis tools}
\label{sec:quanti}

The normalized Shannon entropy applied to two different PDFs and the maximum Lyapunov exponent along with the mean period's lengths are the quantifiers employed here to estimate the system's properties. The entropies   help us to evaluate the two properties that determine the randomness degree, the equiprobability among all possible values and the statistical independence between consecutive values, while the MLE determines the presence of chaos \footnote{A positive MLE is a necessary condition for the presence of chaos.}. 
 
\subsubsection{Period Lengths Analysis}
Using $n$ bits to represent the state variables of a $D$-dimensional system the maximum theoretical period $T_{max}$ that can be reached is $T_{max}=2^{D.n}$. Actually, the periods obtained are much lower than the maximum and are heavily dependent on the IC.

We have developed an algorithm that emulates the operation of the system in a digital environment. One task of this code is to analyze the reached period when starting iteration from each initial condition using different precisions in a fixed-point architecture. Each seed could converge to a limit cycle, or it could be one value of the limit cycle itself. This procedure was repeated for all the initial conditions to obtain the attraction domain scheme of the
system.

\subsubsection{Quantifiers of Randomness}
\label{cu_ran}

The new quantifiers proposed here bring us information about the degree of randomness, that is not available in ``pass-nonpass" tests like those used as standard to evaluate random number generators.

Based on results of previous research \cite{DeMicco2008,Antonelli2016,DeMicco2011} the normalized Shannon entropy was adopted as quantifier to characterize determinism and stochasticity of the generated sequences. This quantifier derives from the Information Theory, and it is a functional of the PDF. Once the PDF is determined the entropy is defined by the very well known normalized Shannon expression:

\begin{equation}
H=-\frac{\sum_{i=1}^{M}{p_i~log~p_i}}{log(M)}, \label{eq:shannon}
\end{equation}

where $M$ is the number of elements of the alphabet.

From a statistical point of view, a chaotic system is the \textsl{source} of a symbolic time series with an alphabet of $M$ symbols. To evaluate entropy one needs first to define a probability distribution function of the time series. It should be noted that the classical probability distribution, here termed PDF based on histograms, takes only into account the ocurrence of values, but it is not able to detect the order of appearance of them. For example, a sequence of random values generated by any noise generator will exhibit a constant PDF between 0 and 1. On the other hand, a saw-tooth sequence will also present a constant PDF between 0 and 1. In both cases the values appear in the series the same number of times but in a different order. This characteristic is crucial because it differentiates a random signal from an entirely predictable one.

There exist different procedures to obtain a PDF \cite{Rosso2009,DeMicco2008,Mischaikow1999,Powell1979,Rosso2001,Pompe2002} and the determination of the best PDF $P$ is a fundamental problem because $P$ and the sample space are inextricably linked. Their applicability depends on particular characteristics of the data, such as stationarity, time series length, variation of the parameters, level of noise contamination, etc. In previous work devoted to PRNGs, the use of two
PDFs was successful for the comparison between different systems. One PDF is the
normalized histogram, and its normalized Shannon
entropy is denoted here $H_{hist}$. The other one is the ordering PDF
proposed by Bandt \& Pompe \cite{Pompe2002} and its  normalized Shannon entropy is  here denoted as $H_{BP}$. By this selection of the PDFs it is possible to cover the two mentioned properties, namely, (1) the probability of occurrence of each element of the alphabet (PDF based on histograms), and (2) the order of the items in the time series (PDF based on Bandt-Pompe technique).
One may consider the statistics of individual symbols or the statistics of sequences of $d$ consecutive symbols. In the first case $P$ is \textsl{non-causal} because it does not change if the outcomes are mixed up and the number of different possible outcomes is $M$. In the second case, the outcome changes if the output is mixed and then one says that $P$ is \textsl{causal}. In the second case the number of different outcomes is equal to $M^d$ and increases rapidly with $d$. Bandt and Pompe made a proposal in \cite{Pompe2002} that is computationally efficient, because it limits the outcomes to $d!$, but retains causal effects.

The representation plane $H_{BP}$ vs $H_{hist}$ is considered in \cite{DeMicco2008}.
A higher value in any of the entropies, $H_{BP}$ and $H_{hist}$, implies an
increase in the uniformity of the involved PDFs. The point
$(1,1)$ represents the ideal point for a system with uniform histogram and
uniform distribution of ordering patterns.
A discussion about the convenience of using these quantifiers is beyond the scope of this paper but there is an extensive literature  \cite{DeMicco2008,Rosso2007C,Martin2006}.

\subsubsection{Maximum Lyapunov Exponent}
The Lyapunov exponents are quantifiers that characterize how the
separation between two trajectories evolves, \cite{Sprott2003}. It
is well known that chaotic behaviors are characterized
mainly by Lyapunov numbers of the dynamic systems. A chaotic behavior requires that one or more
Lyapunov numbers to be greater than zero. Otherwise, the system is stable. In this paper, we
employ the maximum Lyapunov number as it is one of the most useful
indicators of chaos.

The distance between trajectories changes in $2^{MLE}$ for each
iteration, on average. If $MLE<0$ the trajectories converge,
this may be due to a fixed point, if $MLE=0$ the trajectories keep
their distance, this may be due to a limit cycle, if $MLE>0$, the
distance between trajectories diverges, and is an indicator of
chaos, \cite{Strotgartz1994}.

In this case we have adopted a non-analytical way to measure it as here only the inputs and outputs of the system are accessible \cite{Sprott2003}.
